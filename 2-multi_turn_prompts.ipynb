{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-turn prompts in conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多轮对话\n",
    "之前我们探讨了如何在单一对话下使用 prompt 来让模型进行输出，但是在实际应用中，我们通常需要多个对话来完成一个任务，例如，在聊天机器人中，用户可能会针对一个问题进行多次对话，以获取更详细的信息。\n",
    "\n",
    "我们先来看看如何进行多轮对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 从环境变量中获取 API 密钥\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "messages = []\n",
    "\n",
    "def get_response(prompt):\n",
    "    global messages\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=messages\n",
    "    )\n",
    "    messages.append(response.choices[0].message)\n",
    "    return 'AI: ' + response.choices[0].message.content + '\\n'\n",
    "\n",
    "# 示例多轮对话\n",
    "single_turn_prompt = \"什么是大语言模型？\"\n",
    "print(single_turn_prompt)\n",
    "print(get_response(single_turn_prompt))\n",
    "\n",
    "follow_up_prompt = \"大语言模型有哪些后训练方式？\"\n",
    "print(follow_up_prompt)\n",
    "print(get_response(follow_up_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 LangChain\n",
    "可以看到我们进行多轮对话的方式就是将每一次的对话存进一个消息列表 messages 中，每次新输入都要将其加入到这个消息列表。那么有没有更方便的方法？有的，我们可以借助 LangChain 的 ConversationBufferMemory 类来实现。\n",
    "\n",
    "在此之前，我们需要简单熟悉一下 LangChain 的基本用法，其实在之前写 prompt 模板时，我们就已经使用过 LangChain 的 PromptTemplate 类了。\n",
    "\n",
    "于是，让我们先对此前学习过的任务进行小小的修改，以便于后续更好地使用 LangChain。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化语言模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"请用一句话解释一下什么是 Multi-Turn Prompts ? \"\n",
    "\n",
    "print(llm.invoke(basic_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"请提供 {topic} 的定义，然后解释其重要性。\"\n",
    ")\n",
    "\n",
    "chain = structured_prompt | llm\n",
    "input_variables = {\"topic\": \"Multi-Turn Prompts\"}\n",
    "print(chain.invoke(input_variables).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们使用了 LangChain 的 ChatOpenAI 类来初始化一个语言模型，并使用 PromptTemplate 类来定义一个模板，然后使用这个模板来生成一个提示。\n",
    "\n",
    "现在我们已经可以用 LangChain 来替代之前的做法了，你可以尝试一下！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Multi-turn prompts\n",
    "接下来，我们可以使用 LangChain 的 ConversationChain 和 ConversationBufferMemory 来管理我们的多轮对话，并针对每轮对话提供 prompt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=\"你好，我在学习关于 LLM 的知识。你能告诉我关于 LLM 的信息吗？\"))\n",
    "print(conversation.predict(input=\"最早的 LLM 是哪一个模型，是哪篇论文提出的？\"))\n",
    "print(conversation.predict(input=\"现在最新的 LLM 进行了哪些技术的优化？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们将 verbose 这个参数设置为 True，你可以更好的看到 ConversationBufferMemory 发挥的作用，将每轮对话都做了存储，也就是上下文信息的存储。\n",
    "\n",
    "实际上，我们在使用 Multi-Turn Prompts 这一技术的时候，可以写的更简洁规整一些，让输入输出看起来更像在使用一个对话机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"请问 Prompt Engineering 属于后训练技术的一种吗？\",\n",
    "    \"Prompt Engineering 和 Fine-tuning 有何区别？\",\n",
    "    \"有哪些后训练技术可以不依赖 GPU 进行？\"\n",
    "]\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "for prompt in prompts:\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {conversation.predict(input=prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们完成了一个可以多轮对话的 Prompt Engineering 的探索，并且模型不仅会对每次输入的 prompt 进行输出，还会根据之前的对话记录中的 prompt 进行分析，这就是 Multi-turn prompts 技术。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
