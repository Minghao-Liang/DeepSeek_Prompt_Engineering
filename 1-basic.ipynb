{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic of Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用 API Key\n",
    "\n",
    "首先我们需要根据 [DeepSeek 官方的指导](https://api-docs.deepseek.com/zh-cn)，调用 DeepSeek 的 API。\n",
    "\n",
    "建议将 api_key 放在本地环境变量中，这样在代码中就不用明文写 api_key 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 从环境变量中获取 API 密钥\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# 定义一个函数，用于获取响应\n",
    "def get_response(prompt):\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始写 prompt\n",
    "\n",
    "接下来，开始写第一个 prompt，提供一句话给模型，让他生成句子，事实上，这也就是我们所说的 Zero-shot Learning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"请用一句话解释一下什么是 Prompt Engineering\"\n",
    "\n",
    "response = get_response(basic_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们还可以给模型提供一个主题，让他对该主题的内容进行生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"请提供 {topic} 的定义，然后解释其重要性。\"\n",
    ")\n",
    "\n",
    "input_variables = {\"topic\": \"Prompt Engineering\"} # Define the input variables\n",
    "prompt = structured_prompt.format(**input_variables)\n",
    "response = get_response(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，我们还可以同时输入多个 prompt 来让模型进行输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"请解释一下 Prompt Engineering 的重要性。\",\n",
    "    \"列出 3 个关于 Prompt Engineering 的例子。\",\n",
    "    \"提供一个解数学题的 prompt。\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nPrompt {i + 1}: \")\n",
    "    print(prompt)\n",
    "    print(\"\\nResponse: \")\n",
    "    print(get_response(prompt).content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第一个 prompt，我们可以与前面单个 prompt 生成的结果进行对比。\n",
    "\n",
    "然后，我们会探索 Prompt Engineering 再更多例子的应用，包括一步步解一道数学题。\n",
    "\n",
    "在此之前，我们需要学习一些写 prompt 很有用的方法。\n",
    "\n",
    "当我们用提示工程覆盖越来越多的示例和应用程序时，下面的元素会很有帮助：\n",
    "\n",
    "- 指令：您希望模型执行的特定任务或指令\n",
    "- 上下文：外部信息或其他上下文，可以引导模型做出更好的响应\n",
    "- 输入数据：我们有兴趣找到答案的输入或问题\n",
    "- 输出指示：输出的类型或格式。\n",
    "\n",
    "我们先不讨论上下文，先从剩下的部分开始，接下来我们会接着用上面元素组成的模板写 prompt。\n",
    "\n",
    "我们可以尝试构建一个问答任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"回答下面的问题：\n",
    "    问题：{question}\n",
    "    答案：\"\"\"\n",
    ")\n",
    "\n",
    "input  = {\"question\": \"什么是量子计算？\"}\n",
    "print(get_response(question_answer_prompt.format(**input)).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然我们也可以尝试做代码生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"annotation\"],\n",
    "    template=\"\"\"\n",
    "    请根据下面的注释写一个 Python 程序。\n",
    "    注释: {annotation}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "input = {\"annotation\": \"# 写一个快速排序函数对一个输入数组从小到大排序，并给出一个测试样例。\"}\n",
    "print(get_response(code_generation_prompt.format(**input)).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相信你对这套模版方式已经有了初步的掌握，接下来我们来看一个解数学题的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_solving_prompt = PromptTemplate(\n",
    "    input_variables=[\"problem\"],\n",
    "    template=\"\"\"一步一步解出下面这道题目：\n",
    "    题目：{problem}\n",
    "    1)\"\"\"\n",
    ")\n",
    "\n",
    "input = {\"problem\": \"若z=1+i，则\\left | z^2 - 2z \\right | = ?\"}\n",
    "print(get_response(problem_solving_prompt.format(**input)).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来拿去和 DeepSeek-R1 的结果思考过程对比一下吧！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
